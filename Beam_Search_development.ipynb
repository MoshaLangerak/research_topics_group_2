{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:33:53.261559Z",
     "start_time": "2024-10-09T12:33:34.533693Z"
    }
   },
   "source": [
    "from collections import deque\n",
    "from queue import PriorityQueue\n",
    "from load_data import make_growth_target_df\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from itertools import combinations"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## aantekeningen\n",
    "\\\n",
    "windowsize is erg bepalend in welke subgroups de hoogste quality measure halen (max 72 vs 250 bij 7 vs 5 windowsize) \\\n",
    "\\\n",
    "windowsize, windowoverlap en aggregate functies moeten nog als input aan beam search toegevoegd worden (voor de quality measure) \\\n",
    "\\\n",
    "we doen nu alleen equal voor classes column maar moet ook niet not equal? (dan neemt mogelijke optie wel gigantish toe (exponentieel??)) \\\n",
    "\\\n",
    "worden de splitpoints voor numerical class nu wel goed berekent en hoeveel bins is optimaal? \\\n",
    "\\\n",
    "is equalsized bins de beste optie? \\\n",
    "\\\n",
    "in de results opslaan hoe groot de subgroup is \\ \n",
    "\\ \n",
    "als je max gebruikt over de z_scores gaat hij dan niet altijd opzoek naar een subgroup waar een gigantische outlier een onderdeel van is?\\\n",
    "\\\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:33:53.354483Z",
     "start_time": "2024-10-09T12:33:53.327383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Timing decorator to profile any function\n",
    "def time_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:39:45.469887Z",
     "start_time": "2024-10-09T12:39:45.332255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gt(a, b):\n",
    "    \"\"\"Returns True if a is greater than b.\"\"\"\n",
    "    return a > b\n",
    "\n",
    "def leeq(a, b):\n",
    "    \"\"\"Returns True if a is less than or equal to b.\"\"\"\n",
    "    return a <= b\n",
    "\n",
    "def eq(a, b):\n",
    "    \"\"\"Returns True if a is equal to b.\"\"\"\n",
    "    return a == b\n",
    "\n",
    "def neq(a, b):\n",
    "    \"\"\"Returns True if a is not equal to b.\"\"\"\n",
    "    return not eq(a, b)\n",
    "\n",
    "def extract_subgroup(descriptors, data, col_index_dict):\n",
    "    \"\"\"Extracts a subgroup of data that matches all provided descriptors.\n",
    "\n",
    "    Args:\n",
    "        descriptors: A list of descriptors, each containing an attribute name, value, and operator.\n",
    "        data: The dataset from which to extract the subgroup.\n",
    "        col_index_dict: A dictionary mapping column names to their indices.\n",
    "\n",
    "    Returns:\n",
    "        A list of rows from the data that match all descriptors.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for row in data:\n",
    "        check = True\n",
    "        for attribute in descriptors:\n",
    "            att_name, descr_value, operator = attribute  # unpack 3 values from attribute\n",
    "            att_index = col_index_dict[att_name]  # get the index for the attribute\n",
    "            value = operator(row[att_index], descr_value)  # apply the operator\n",
    "\n",
    "            if not value:  # if any descriptor does not match\n",
    "                check = False\n",
    "                break\n",
    "\n",
    "        if check:  # if all descriptors match\n",
    "            result.append(row)  # add the row to the result\n",
    "\n",
    "    return result\n",
    "\n",
    "def refin(seed, data, types, nr_bins, descr_indices, index_col_dict, col_index_dict):\n",
    "    \"\"\"Generates new descriptor sets by refining the seed descriptors.\n",
    "\n",
    "    Args:\n",
    "        seed: The initial set of descriptors to refine.\n",
    "        data: The dataset used for extracting subgroups.\n",
    "        types: The types of each column in the dataset.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        descr_indices: The indices of potential descriptors.\n",
    "        index_col_dict: A dictionary mapping index to column names.\n",
    "        col_index_dict: A dictionary mapping column names to indices.\n",
    "\n",
    "    Returns:\n",
    "        A list of new descriptor sets created by refining the seed.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    used_descr = [col_index_dict[i[0]] for i in seed]  # get used descriptors\n",
    "    not_used_indices = descr_indices[:]  # make a copy of descriptor indices\n",
    "    # Filter out indices that are already used or are numeric types\n",
    "    not_used_indices = [i for i in not_used_indices if i not in used_descr or types[i] == \"numeric\"]\n",
    "\n",
    "    for i in not_used_indices:\n",
    "        aux = list(seed)[:]  # copy the seed\n",
    "\n",
    "        if types[i] == 'numeric':\n",
    "            s = extract_subgroup(seed, data, col_index_dict)  # extract subgroup based on seed\n",
    "            all_values = [float(entry[i]) for entry in s]  # get all values for the numeric attribute\n",
    "            all_values = sorted(all_values)  # sort values\n",
    "            n = len(all_values)\n",
    "            # Create split points for binning\n",
    "            split_points = [all_values[math.floor(j * (n/nr_bins))] for j in range(1, nr_bins)]\n",
    "            for s in split_points:\n",
    "                func1 = leeq\n",
    "                func2 = gt\n",
    "\n",
    "                # Create two new descriptors for each split point\n",
    "                local0 = aux[:]\n",
    "                local0.append((index_col_dict[i], s, func1))  # descriptor for less than or equal\n",
    "                res.append(local0)\n",
    "\n",
    "                local1 = aux[:]\n",
    "                local1.append((index_col_dict[i], s, func2))  # descriptor for greater than\n",
    "                res.append(local1)\n",
    "\n",
    "        elif types[i] == 'binary':\n",
    "            func = eq  # equality function for binary descriptors\n",
    "            local0 = aux[:]\n",
    "            local0.append((index_col_dict[i], 0, func))  # descriptor for value 0\n",
    "            local1 = aux[:]\n",
    "            local1.append((index_col_dict[i], 1, func))  # descriptor for value 1\n",
    "            res.append(local0)\n",
    "            res.append(local1)\n",
    "\n",
    "        else:  # nominal attributes\n",
    "            all_values = [entry[i] for entry in data]  # get all unique values\n",
    "            for j in set(all_values):\n",
    "                func1 = eq\n",
    "                func2 = neq\n",
    "                local0 = aux[:]\n",
    "                local0.append((index_col_dict[i], j, func1))  # descriptor for equality\n",
    "                res.append(local0)\n",
    "                # local1 = aux[:]\n",
    "                # local1.append((index_col_dict[i], j, func2))  # descriptor for not equality\n",
    "                # res.append(local1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def put_item_in_queue(queue, quality, descriptor, size=0, t=0):\n",
    "    \"\"\"Adds an item to a priority queue based on its quality.\n",
    "\n",
    "    Args:\n",
    "        queue: The priority queue to which the item will be added.\n",
    "        quality: The quality measure of the item.\n",
    "        descriptor: The descriptor associated with the item.\n",
    "        size: The size of the subgroup represented by the descriptor.\n",
    "    \"\"\"\n",
    "    if queue.full():  # if the queue is full\n",
    "        min_quality, min_descriptor, min_size, min_t = queue.get()  # get the lowest quality item\n",
    "        if min_quality >= quality:  # if the new item is not better, put the old one back\n",
    "            queue.put((min_quality, min_descriptor, min_size, min_t))\n",
    "        else:  # otherwise, add the new item\n",
    "            queue.put((quality, descriptor, size, t))\n",
    "    else:\n",
    "        queue.put((quality, descriptor, size, t))  # add new item to the queue\n",
    "\n",
    "def categorize_columns_in_order(df, att_columns):\n",
    "    \"\"\"Categorizes columns of a DataFrame into numeric, binary, and nominal types.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing the data.\n",
    "        att_columns: The columns to categorize.\n",
    "\n",
    "    Returns:\n",
    "        A list of column types corresponding to the provided attribute columns.\n",
    "    \"\"\"\n",
    "    column_types = []  # List to store the categories in order\n",
    "\n",
    "    for col in att_columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):  # Check if the column is numeric\n",
    "            column_types.append('numeric')\n",
    "        elif df[col].nunique() == 2:  # Check for binary columns\n",
    "            column_types.append('binary')\n",
    "        else:  # Otherwise, treat it as nominal\n",
    "            column_types.append('nominal')\n",
    "\n",
    "    return column_types\n",
    "\n",
    "def make_rolling_windows(growth_target, window_size):\n",
    "    \"\"\"Creates rolling windows for the target data.\n",
    "\n",
    "    Args:\n",
    "        growth_target: The target data for which to create rolling windows.\n",
    "        window_size: The size of each window.\n",
    "\n",
    "    Returns:\n",
    "        A new array of rolling windows.\n",
    "    \"\"\"\n",
    "    return np.lib.stride_tricks.sliding_window_view(growth_target, window_shape=window_size)[::window_size]\n",
    "\n",
    "def quality_measure(targets_subgroup, targets_baseline,\n",
    "                    aggregate_func_window=np.mean, aggregate_func=np.max):\n",
    "    \"\"\"Calculates a quality measure for a subgroup compared to a baseline.\n",
    "\n",
    "    Args:\n",
    "        targets_subgroup: list with timeseries where the timeseries are divided in windows (list with lists with lists)\n",
    "        targets_baseline: list with baseline target values in windows (list with lists).\n",
    "        aggregate_func_window: Function to aggregate over windows (default: mean).\n",
    "        aggregate_func: Function to aggregate the final quality measure (default: max).\n",
    "\n",
    "    Returns:\n",
    "        A quality score representing the difference between the subgroup and baseline.\n",
    "    \"\"\"\n",
    "    # Aggregate points in each window for each timeseries in the subgroup\n",
    "    subgroup_aggregated_windows = aggregate_func_window(targets_subgroup, axis=2)\n",
    "\n",
    "    # Calculate mean values for the baseline and subgroup for each window\n",
    "    baseline_means = np.mean(targets_baseline, axis=1)\n",
    "    subgroup_means = np.mean(subgroup_aggregated_windows, axis=0)\n",
    "\n",
    "    # Calculate absolute differences between means for each window\n",
    "    abs_diff_mean = np.abs(subgroup_means - baseline_means)\n",
    "\n",
    "    # Calculate standard error of the subgroup for each window\n",
    "    subgroup_std = np.std(subgroup_aggregated_windows, axis=0)\n",
    "    standard_error_subgroup = subgroup_std / np.sqrt(len(targets_subgroup))\n",
    "\n",
    "    # Calculate z-scores\n",
    "    z_scores = np.divide(abs_diff_mean, standard_error_subgroup, where=standard_error_subgroup != 0)\n",
    "\n",
    "    # Calculate the final quality score\n",
    "    quality_score = aggregate_func(z_scores)\n",
    "\n",
    "    return quality_score\n",
    "\n",
    "\n",
    "def beam_search(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size, max_subgroup_size=100000):\n",
    "    \"\"\"Performs beam search to identify optimal descriptors for subgroups.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to analyze.\n",
    "        targets_baseline: The baseline target values for comparison.\n",
    "        column_names: The names of the columns in the dataset.\n",
    "        beam_width: The number of descriptors to keep at each depth level.\n",
    "        beam_depth: The maximum depth of the beam search.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        nr_saved: The number of best results to save.\n",
    "        subgroup_size: The minimum size of a subgroup to consider.\n",
    "        target: The target variable for which to evaluate subgroups.\n",
    "        types: The types of each column in the dataset (e.g., numeric, binary, nominal).\n",
    "        window_size: The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        A list of the best descriptor sets found during the search.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for indexing columns by name and vice versa\n",
    "    index_col_dict = {i: col for i, col in enumerate(column_names)}\n",
    "    col_index_dict = {col: i for i, col in enumerate(column_names)}\n",
    "    target_ind = column_names.index(target)  # Get index of the target column\n",
    "    att_indices = list(range(len(column_names)))  # Create a list of all indices\n",
    "    att_indices.remove(target_ind)  # Remove the target index from attribute indices\n",
    "\n",
    "    # Prepare data windows with rolling windows for the target variable\n",
    "    data_windows = []\n",
    "    for row in data:\n",
    "        new_row = row[:]  # Create a copy of the row\n",
    "        new_row[target_ind] = make_rolling_windows(row[target_ind], window_size)  # Apply rolling window\n",
    "        data_windows.append(new_row)  # Add the new row to data_windows\n",
    "\n",
    "    # Update the data and baseline targets to use rolling windows\n",
    "    data = data_windows\n",
    "    targets_baseline = make_rolling_windows(targets_baseline, window_size)\n",
    "\n",
    "    # Initialize a deque for the beam search and a priority queue for results\n",
    "    beam_queue = deque([()])  # Start with an empty seed\n",
    "    results = PriorityQueue(nr_saved)  # Queue to hold the best results\n",
    "\n",
    "    # Iterate through each depth of the beam search\n",
    "    for depth in range(beam_depth):\n",
    "        beam = PriorityQueue(beam_width)  # Initialize a new beam for this depth\n",
    "\n",
    "        # While there are seeds in the beam queue\n",
    "        while bool(beam_queue):\n",
    "            seed = beam_queue.popleft()  # Get the next seed descriptor\n",
    "            descriptor_set = refin(seed, data, types, nr_bins, att_indices, index_col_dict, col_index_dict)  # Refine descriptors based on seed\n",
    "\n",
    "            # Evaluate each descriptor set generated\n",
    "            for descriptor in descriptor_set:\n",
    "                subgroup = extract_subgroup(descriptor, data, col_index_dict)  # Extract subgroup for the current descriptor\n",
    "                if len(subgroup) >= subgroup_size and len(subgroup)<max_subgroup_size:  # Ensure subgroup is large enough\n",
    "                    targets_subgroup = [i[target_ind] for i in subgroup]  # Extract target values for the subgroup\n",
    "                    quality_result = quality_measure(targets_subgroup, targets_baseline)  # Calculate quality measure\n",
    "                    put_item_in_queue(results, quality_result, tuple(descriptor), len(subgroup))  # Add to results queue\n",
    "                    put_item_in_queue(beam, quality_result, tuple(descriptor))  # Add to the current beam\n",
    "\n",
    "        # After processing the beam, update the beam queue with new combinations\n",
    "        while not beam.empty():\n",
    "            new_combination = beam.get()  # Get the highest quality descriptor from the beam\n",
    "            new_combination = new_combination[1]  # Extract the descriptor from the tuple\n",
    "            beam_queue.append(new_combination)  # Add it to the next depth of the beam search\n",
    "\n",
    "    # Compile results into a list and reverse to have the best results first\n",
    "    results_list = []\n",
    "    while not results.empty():\n",
    "        item = results.get()  # Get items from the results queue\n",
    "        results_list.append(item)  # Add to the results list\n",
    "    results_list.reverse()  # Reverse the list to have best results first\n",
    "\n",
    "    return results_list  # Return the list of best descriptor sets\n",
    "\n",
    "\n",
    "def filter_df_on_descriptors(df, descriptors):\n",
    "    \"\"\"Filters a DataFrame based on specified descriptors.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to filter.\n",
    "        descriptors: A list of descriptors, each containing an attribute name, value, and operator.\n",
    "\n",
    "    Returns:\n",
    "        A filtered DataFrame where all descriptors hold true.\n",
    "    \"\"\"\n",
    "    # Loop through each descriptor to apply filtering\n",
    "    for desc in descriptors:\n",
    "        # Apply the operator defined in the descriptor to filter the DataFrame\n",
    "        df = df[df[desc[0]].apply(lambda x: desc[2](x, desc[1]))]  # Filter based on the descriptor\n",
    "    return df  # Return the filtered DataFrame\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "################# BEAM SEARCH WITH (SELF MADE) CONSTRAINT #################################################\n",
    "### it is finished but not perfect and complicated, maybe finding something in the literature is better ###\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def get_all_descriptors(pq, index):\n",
    "    \"\"\"Retrieve all descriptors from a priority queue without altering its contents.\n",
    "\n",
    "    Args:\n",
    "        pq: A priority queue containing descriptor tuples.\n",
    "\n",
    "    Returns:\n",
    "        A list of descriptors.\n",
    "    \"\"\"\n",
    "    temp_items = []  # Temporary list to store all items from the queue\n",
    "    info = []  # List to store descriptors\n",
    "\n",
    "    # Step 1: Retrieve all items from the queue\n",
    "    while not pq.empty():\n",
    "        item = pq.get()  # Get item from the queue\n",
    "        temp_items.append(item)  # Store the item temporarily\n",
    "        info.append(item[index])  # Extract and store the descriptor\n",
    "\n",
    "    # Step 2: Put all items back into the queue to maintain its original state\n",
    "    for item in temp_items:\n",
    "        pq.put(item)\n",
    "\n",
    "    # Step 3: Return the list of all descriptors\n",
    "    return info\n",
    "\n",
    "\n",
    "def are_descriptors_similar(descriptor1, pq):\n",
    "    \"\"\"Check if the given descriptor is similar to any descriptors in the priority queue.\n",
    "\n",
    "    Args:\n",
    "        descriptor1: The first descriptor to compare (a list of (metric, value, func) tuples).\n",
    "        pq: A priority queue containing other descriptors.\n",
    "\n",
    "    Returns:\n",
    "        True if a similar descriptor is found in the queue, False otherwise.\n",
    "    \"\"\"\n",
    "    tolerance = 0.25  # Tolerance range for numeric comparison\n",
    "    desc1_dict = {metric: (value, func) for metric, value, func in descriptor1}  # Convert descriptor1 to a dictionary\n",
    "    descriptor_list = get_all_descriptors(pq, 1)  # Get all descriptors from the queue\n",
    "\n",
    "    # Iterate through each descriptor in the list\n",
    "    for descriptor2 in descriptor_list:\n",
    "        desc2_dict = {metric: (value, func) for metric, value, func in descriptor2}  # Convert descriptor2 to a dictionary\n",
    "\n",
    "        # If the length of descriptors doesn't match, skip\n",
    "        if len(desc1_dict) != len(desc2_dict):\n",
    "            continue\n",
    "\n",
    "        all_metrics_match = True  # Flag to track if all metrics match\n",
    "\n",
    "        # Check each metric in descriptor1\n",
    "        for metric in desc1_dict:\n",
    "            if metric in desc2_dict:\n",
    "                value1, func1 = desc1_dict[metric]\n",
    "                value2, func2 = desc2_dict[metric]\n",
    "\n",
    "                # Check if the functions are the same\n",
    "                if func1 != func2:\n",
    "                    all_metrics_match = False\n",
    "                    break\n",
    "\n",
    "                # Check if numeric values are within tolerance range\n",
    "                if (not isinstance(value1, str)) and abs(value1 - value2) > abs(tolerance * value1):\n",
    "                    all_metrics_match = False\n",
    "                    break\n",
    "\n",
    "                # For strings, check if values are exactly the same\n",
    "                if isinstance(value1, str) and value1 != value2:\n",
    "                    all_metrics_match = False\n",
    "                    break\n",
    "            else:\n",
    "                # If metric in descriptor1 is not in descriptor2, they are not similar\n",
    "                all_metrics_match = False\n",
    "                break\n",
    "\n",
    "        # If all metrics match, return True (descriptors are similar)\n",
    "        if all_metrics_match:\n",
    "            return True\n",
    "\n",
    "    # If no similar descriptors are found, return False\n",
    "    return False\n",
    "\n",
    "def beam_search_with_constraint_descriptor(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size, max_subgroup_size=100000):\n",
    "    \"\"\"Performs beam search with a constraint to avoid adding similar descriptors.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to analyze.\n",
    "        targets_baseline: The baseline target values for comparison.\n",
    "        column_names: The names of the columns in the dataset.\n",
    "        beam_width: The number of descriptors to keep at each depth level.\n",
    "        beam_depth: The maximum depth of the beam search.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        nr_saved: The number of best results to save.\n",
    "        subgroup_size: The minimum size of a subgroup to consider.\n",
    "        target: The target variable for which to evaluate subgroups.\n",
    "        types: The types of each column in the dataset (e.g., numeric, binary, nominal).\n",
    "        window_size: The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        A list of the best descriptor sets found during the search, ensuring no similar descriptors.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for indexing columns by name and vice versa\n",
    "    index_col_dict = {i: col for i, col in enumerate(column_names)}\n",
    "    col_index_dict = {col: i for i, col in enumerate(column_names)}\n",
    "    target_ind = column_names.index(target)  # Get index of the target column\n",
    "    att_indices = list(range(len(column_names)))  # Create a list of all indices\n",
    "    att_indices.remove(target_ind)  # Remove the target index from attribute indices\n",
    "\n",
    "    # Prepare data windows with rolling windows for the target variable\n",
    "    data_windows = []\n",
    "    for row in data:\n",
    "        new_row = row[:]  # Create a copy of the row\n",
    "        new_row[target_ind] = make_rolling_windows(row[target_ind], window_size)  # Apply rolling window\n",
    "        data_windows.append(new_row)  # Add the new row to data_windows\n",
    "\n",
    "    # Update the data and baseline targets to use rolling windows\n",
    "    data = data_windows\n",
    "    targets_baseline = make_rolling_windows(targets_baseline, window_size)\n",
    "\n",
    "    # Initialize a deque for the beam search and a priority queue for results\n",
    "    beam_queue = deque([()])  # Start with an empty seed\n",
    "    results = PriorityQueue(nr_saved)  # Queue to hold the best results\n",
    "    results.put((0, [(0, 0, 0)], 0))  # Add a dummy descriptor to initialize\n",
    "\n",
    "    # Iterate through each depth of the beam search\n",
    "    for depth in range(beam_depth):\n",
    "        beam = PriorityQueue(beam_width)  # Initialize a new beam for this depth\n",
    "\n",
    "        # While there are seeds in the beam queue\n",
    "        while bool(beam_queue):\n",
    "            seed = beam_queue.popleft()  # Get the next seed descriptor\n",
    "            descriptor_set = refin(seed, data, types, nr_bins, att_indices, index_col_dict, col_index_dict)  # Refine descriptors based on seed\n",
    "\n",
    "            # Evaluate each descriptor set generated\n",
    "            for descriptor in descriptor_set:\n",
    "                subgroup = extract_subgroup(descriptor, data, col_index_dict)  # Extract subgroup for the current descriptor\n",
    "                if len(subgroup) >= subgroup_size and len(subgroup) < max_subgroup_size and not are_descriptors_similar(descriptor, results):  # Ensure subgroup is large enough and descriptor is not similar\n",
    "                    targets_subgroup = [i[target_ind] for i in subgroup]  # Extract target values for the subgroup\n",
    "                    quality_result = quality_measure(targets_subgroup, targets_baseline)  # Calculate quality measure\n",
    "                    put_item_in_queue(results, quality_result, tuple(descriptor), len(subgroup))  # Add to results queue\n",
    "                    put_item_in_queue(beam, quality_result, tuple(descriptor))  # Add to the current beam\n",
    "\n",
    "        # After processing the beam, update the beam queue with new combinations\n",
    "        while not beam.empty():\n",
    "            new_combination = beam.get()  # Get the highest quality descriptor from the beam\n",
    "            new_combination = new_combination[1]  # Extract the descriptor from the tuple\n",
    "            beam_queue.append(new_combination)  # Add it to the next depth of the beam search\n",
    "\n",
    "    # Compile results into a list and reverse to have the best results first\n",
    "    results_list = []\n",
    "    while not results.empty():\n",
    "        item = results.get()  # Get items from the results queue\n",
    "        results_list.append(item)  # Add to the results list\n",
    "    results_list.reverse()  # Reverse the list to have best results first\n",
    "\n",
    "    return results_list  # Return the list of best descriptor sets\n",
    "\n",
    "def extract_subgroup_index(descriptors, data, col_index_dict):\n",
    "    \"\"\"Extracts a subgroup of data that matches all provided descriptors.\n",
    "\n",
    "    Args:\n",
    "        descriptors: A list of descriptors, each containing an attribute name, value, and operator.\n",
    "        data: The dataset from which to extract the subgroup.\n",
    "        col_index_dict: A dictionary mapping column names to their indices.\n",
    "\n",
    "    Returns:\n",
    "        A list of rows from the data that match all descriptors.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    index_list = []\n",
    "    for i, row in enumerate(data):\n",
    "        check = True\n",
    "        for attribute in descriptors:\n",
    "            att_name, descr_value, operator = attribute  # unpack 3 values from attribute\n",
    "            att_index = col_index_dict[att_name]  # get the index for the attribute\n",
    "            value = operator(row[att_index], descr_value)  # apply the operator\n",
    "    \n",
    "            if not value:  # if any descriptor does not match\n",
    "                check = False\n",
    "                break\n",
    "    \n",
    "        if check:  # if all descriptors match\n",
    "            result.append(row)  # add the row to the result\n",
    "            index_list.append(i)\n",
    "    \n",
    "    return result, index_list\n",
    "\n",
    "def beam_search_with_constraint_cover(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size, max_subgroup_size=100000):\n",
    "    \"\"\"Performs beam search with a constraint to avoid adding similar descriptors.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to analyze.\n",
    "        targets_baseline: The baseline target values for comparison.\n",
    "        column_names: The names of the columns in the dataset.\n",
    "        beam_width: The number of descriptors to keep at each depth level.\n",
    "        beam_depth: The maximum depth of the beam search.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        nr_saved: The number of best results to save.\n",
    "        subgroup_size: The minimum size of a subgroup to consider.\n",
    "        target: The target variable for which to evaluate subgroups.\n",
    "        types: The types of each column in the dataset (e.g., numeric, binary, nominal).\n",
    "        window_size: The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        A list of the best descriptor sets found during the search, ensuring no similar descriptors.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for indexing columns by name and vice versa\n",
    "    index_col_dict = {i: col for i, col in enumerate(column_names)}\n",
    "    col_index_dict = {col: i for i, col in enumerate(column_names)}\n",
    "    target_ind = column_names.index(target)  # Get index of the target column\n",
    "    att_indices = list(range(len(column_names)))  # Create a list of all indices\n",
    "    att_indices.remove(target_ind)  # Remove the target index from attribute indices\n",
    "\n",
    "    # Prepare data windows with rolling windows for the target variable\n",
    "    data_windows = []\n",
    "    for row in data:\n",
    "        new_row = row[:]  # Create a copy of the row\n",
    "        new_row[target_ind] = make_rolling_windows(row[target_ind], window_size)  # Apply rolling window\n",
    "        data_windows.append(new_row)  # Add the new row to data_windows\n",
    "\n",
    "    # Update the data and baseline targets to use rolling windows\n",
    "    data = data_windows\n",
    "    targets_baseline = make_rolling_windows(targets_baseline, window_size)\n",
    "\n",
    "    # Initialize a deque for the beam search and a priority queue for results\n",
    "    beam_queue = deque([()])  # Start with an empty seed\n",
    "    results = PriorityQueue(nr_saved)  # Queue to hold the best results\n",
    "    results.put((0, [(0, 0, 0)], [-1],0))  # Add a dummy descriptor to initialize\n",
    "    cover_weight = 0.5\n",
    "    max_quality = 1\n",
    "    # Iterate through each depth of the beam search\n",
    "    for depth in range(beam_depth):\n",
    "        beam = PriorityQueue(beam_width)  # Initialize a new beam for this depth\n",
    "\n",
    "        # While there are seeds in the beam queue\n",
    "        while bool(beam_queue):\n",
    "            seed = beam_queue.popleft()  # Get the next seed descriptor\n",
    "            descriptor_set = refin(seed, data, types, nr_bins, att_indices, index_col_dict, col_index_dict)  # Refine descriptors based on seed\n",
    "\n",
    "            # Evaluate each descriptor set generated\n",
    "            for descriptor in descriptor_set:\n",
    "                subgroup, index_list = extract_subgroup_index(descriptor, data, col_index_dict)  # Extract subgroup for the current descriptor\n",
    "                if len(subgroup) >= subgroup_size and len(subgroup) < max_subgroup_size:  # Ensure subgroup is large enough and descriptor is not similar\n",
    "                    targets_subgroup = [i[target_ind] for i in subgroup]  # Extract target values for the subgroup\n",
    "                    quality_result = quality_measure(targets_subgroup, targets_baseline)  # Calculate quality measure\n",
    "                    cover_score = calc_cover_score(index_list, results)\n",
    "                    final_score = (cover_weight * (cover_score*quality_result)) + ((1-cover_weight) * quality_result)\n",
    "                    # print(descriptor)\n",
    "                    # print(final_score, quality_result, cover_score)\n",
    "                    # print('\\n')\n",
    "                    put_item_in_queue(results, final_score, tuple(descriptor), index_list, quality_result)  # Add to results queue\n",
    "                    put_item_in_queue(beam, final_score, tuple(descriptor))  # Add to the current beam\n",
    "\n",
    "        # After processing the beam, update the beam queue with new combinations\n",
    "        while not beam.empty():\n",
    "            new_combination = beam.get()  # Get the highest quality descriptor from the beam\n",
    "            new_combination = new_combination[1]  # Extract the descriptor from the tuple\n",
    "            beam_queue.append(new_combination)  # Add it to the next depth of the beam search\n",
    "\n",
    "    # Compile results into a list and reverse to have the best results first\n",
    "    results_list = []\n",
    "    while not results.empty():\n",
    "        item = results.get()  # Get items from the results queue\n",
    "        results_list.append(item)  # Add to the results list\n",
    "    results_list.reverse()  # Reverse the list to have best results first\n",
    "\n",
    "    return results_list  # Return the list of best descriptor sets\n",
    "\n",
    "def calc_cover_score(new_index_list, results_pq, alpha):\n",
    "    all_result_index_list = get_all_descriptors(results_pq, 2)\n",
    "    all_cover_scores = []\n",
    "    for result_index_list in all_result_index_list:\n",
    "        covered_tuples = [t for t in new_index_list if t in result_index_list]\n",
    "        # if not new_index_list:  # If subgroup G is empty\n",
    "        #     cover_score =  0\n",
    "        # else:\n",
    "        cover_score = alpha ** len(covered_tuples)\n",
    "        all_cover_scores.append(cover_score)\n",
    "        print(all_cover_scores, len(covered_tuples))\n",
    "    return min(all_cover_scores)\n",
    "\n",
    "\n",
    "\n",
    "def calc_cover_score(new_index_list, results_pq, alpha):\n",
    "    all_result_index_list = get_all_descriptors(results_pq, 2)\n",
    "    alpha_scores = []\n",
    "    for index in new_index_list:\n",
    "        count_index = sum(sublist.count(index) for sublist in all_result_index_list)\n",
    "        alpha_scores.append(alpha ** count_index)\n",
    "    cover_score = sum(alpha_scores) / len(new_index_list)\n",
    "    return cover_score\n",
    "\n",
    "\n",
    "def calc_cover_score(new_index_list, results_pq):\n",
    "    all_result_index_list = get_all_descriptors(results_pq, 2)\n",
    "    new_index_set = set(new_index_list)  # Convert to set for faster lookups\n",
    "    size_new_index = len(new_index_set)   # Pre-calculate size\n",
    "    all_cover_scores = []\n",
    "\n",
    "    for result_index_list in all_result_index_list:\n",
    "        result_index_set = set(result_index_list)  # Convert to set\n",
    "        covered_tuples = new_index_set.intersection(result_index_set)  # Get intersection\n",
    "        size_result_index = len(result_index_set)  # Size of result index list\n",
    "        size = max(size_new_index, size_result_index)  # Max size for cover score calculation\n",
    "        cover_score = len(covered_tuples) / size  # Calculate cover score\n",
    "        all_cover_scores.append(cover_score)\n",
    "\n",
    "    return 1 - max(all_cover_scores)\n",
    "\n",
    "\n",
    "def beam_search_with_constraint_descriptor2(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size, max_subgroup_size=100000):\n",
    "    \"\"\"Performs beam search with a constraint to avoid adding similar descriptors.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to analyze.\n",
    "        targets_baseline: The baseline target values for comparison.\n",
    "        column_names: The names of the columns in the dataset.\n",
    "        beam_width: The number of descriptors to keep at each depth level.\n",
    "        beam_depth: The maximum depth of the beam search.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        nr_saved: The number of best results to save.\n",
    "        subgroup_size: The minimum size of a subgroup to consider.\n",
    "        target: The target variable for which to evaluate subgroups.\n",
    "        types: The types of each column in the dataset (e.g., numeric, binary, nominal).\n",
    "        window_size: The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        A list of the best descriptor sets found during the search, ensuring no similar descriptors.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for indexing columns by name and vice versa\n",
    "    index_col_dict = {i: col for i, col in enumerate(column_names)}\n",
    "    col_index_dict = {col: i for i, col in enumerate(column_names)}\n",
    "    target_ind = column_names.index(target)  # Get index of the target column\n",
    "    att_indices = list(range(len(column_names)))  # Create a list of all indices\n",
    "    att_indices.remove(target_ind)  # Remove the target index from attribute indices\n",
    "\n",
    "    # Prepare data windows with rolling windows for the target variable\n",
    "    data_windows = []\n",
    "    for row in data:\n",
    "        new_row = row[:]  # Create a copy of the row\n",
    "        new_row[target_ind] = make_rolling_windows(row[target_ind], window_size)  # Apply rolling window\n",
    "        data_windows.append(new_row)  # Add the new row to data_windows\n",
    "\n",
    "    # Update the data and baseline targets to use rolling windows\n",
    "    data = data_windows\n",
    "    targets_baseline = make_rolling_windows(targets_baseline, window_size)\n",
    "\n",
    "    # Initialize a deque for the beam search and a priority queue for results\n",
    "    beam_queue = deque([()])  # Start with an empty seed\n",
    "    results = PriorityQueue(nr_saved)  # Queue to hold the best results\n",
    "    results.put((0, [(0, 0, 0)], 0, 0))  # Add a dummy descriptor to initialize\n",
    "\n",
    "    # Iterate through each depth of the beam search\n",
    "    for depth in range(beam_depth):\n",
    "        beam = PriorityQueue(beam_width)  # Initialize a new beam for this depth\n",
    "\n",
    "        # While there are seeds in the beam queue\n",
    "        while bool(beam_queue):\n",
    "            seed = beam_queue.popleft()  # Get the next seed descriptor\n",
    "            descriptor_set = refin(seed, data, types, nr_bins, att_indices, index_col_dict, col_index_dict)  # Refine descriptors based on seed\n",
    "\n",
    "            # Evaluate each descriptor set generated\n",
    "            for descriptor in descriptor_set:\n",
    "                subgroup = extract_subgroup(descriptor, data, col_index_dict)  # Extract subgroup for the current descriptor\n",
    "                if len(subgroup) >= subgroup_size and len(subgroup) < max_subgroup_size:  # Ensure subgroup is large enough and descriptor is not similar\n",
    "                    targets_subgroup = [i[target_ind] for i in subgroup]  # Extract target values for the subgroup\n",
    "                    quality_result = quality_measure(targets_subgroup, targets_baseline)  # Calculate quality measure\n",
    "                    if not are_descriptors_similar2(quality_result, descriptor, results):\n",
    "                        put_item_in_queue(results, quality_result, tuple(descriptor), len(subgroup))  # Add to results queue\n",
    "                        put_item_in_queue(beam, quality_result, tuple(descriptor))  # Add to the current beam\n",
    "\n",
    "        # After processing the beam, update the beam queue with new combinations\n",
    "        while not beam.empty():\n",
    "            new_combination = beam.get()  # Get the highest quality descriptor from the beam\n",
    "            new_combination = new_combination[1]  # Extract the descriptor from the tuple\n",
    "            beam_queue.append(new_combination)  # Add it to the next depth of the beam search\n",
    "\n",
    "    # Compile results into a list and reverse to have the best results first\n",
    "    results_list = []\n",
    "    while not results.empty():\n",
    "        item = results.get()  # Get items from the results queue\n",
    "        results_list.append(item)  # Add to the results list\n",
    "    results_list.reverse()  # Reverse the list to have best results first\n",
    "\n",
    "    return results_list  # Return the list of best descriptor sets\n",
    "\n",
    "def are_descriptors_similar2(quality, descriptor1, pq):\n",
    "    \"\"\"Check if the given descriptor is similar to any descriptors in the priority queue.\n",
    "\n",
    "    Args:\n",
    "        descriptor1: The first descriptor to compare (a list of (metric, value, func) tuples).\n",
    "        pq: A priority queue containing other descriptors.\n",
    "\n",
    "    Returns:\n",
    "        True if a similar descriptor is found in the queue, False otherwise.\n",
    "    \"\"\"\n",
    "    tolerance = 0.1 # Tolerance range for numeric comparison\n",
    "    desc1_dict = {metric: (value, func) for metric, value, func in descriptor1}  # Convert descriptor1 to a dictionary\n",
    "    descriptor_list = get_all_descriptors(pq, 1)\n",
    "    quality_list = get_all_descriptors(pq, 0)\n",
    "    #print(quality_list)\n",
    "    quality_diff = min([abs(quality - q) for q in quality_list])\n",
    "    if quality_diff > 5:\n",
    "        return False\n",
    "    match_count=0\n",
    "    # Iterate through each descriptor in the list\n",
    "    for descriptor2 in descriptor_list:\n",
    "        desc2_dict = {metric: (value, func) for metric, value, func in descriptor2}  # Convert descriptor2 to a dictionary\n",
    "\n",
    "        # If the length of descriptors doesn't match, skip\n",
    "        if len(desc1_dict) != len(desc2_dict):\n",
    "            continue\n",
    "\n",
    "        all_metrics_match = True  # Flag to track if all metrics match\n",
    "\n",
    "        # Check each metric in descriptor1\n",
    "        for metric in desc1_dict:\n",
    "            if match_count>=2:\n",
    "                return True\n",
    "            if metric in desc2_dict:\n",
    "                value1, func1 = desc1_dict[metric]\n",
    "                value2, func2 = desc2_dict[metric]\n",
    "\n",
    "                # Check if the functions are the same\n",
    "                if func1 != func2:\n",
    "                    all_metrics_match = False\n",
    "                    continue\n",
    "\n",
    "                # Check if numeric values are within tolerance range\n",
    "                if (not isinstance(value1, str)) and abs(value1 - value2) > abs(tolerance * value1):\n",
    "                    all_metrics_match = False\n",
    "                    continue\n",
    "\n",
    "                # For strings, check if values are exactly the same\n",
    "                if isinstance(value1, str) and value1 != value2:\n",
    "                    all_metrics_match = False\n",
    "                    continue\n",
    "                else:\n",
    "                    match_count+=1\n",
    "                    all_metrics_match=True\n",
    "            else:\n",
    "                # If metric in descriptor1 is not in descriptor2, they are not similar\n",
    "                all_metrics_match = False\n",
    "                continue\n",
    "\n",
    "        # If all metrics match, return True (descriptors are similar)\n",
    "        if all_metrics_match and match_count>=2:\n",
    "            return True\n",
    "\n",
    "    # If no similar descriptors are found, return False\n",
    "    return False\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:35:11.403179Z",
     "start_time": "2024-10-09T12:33:55.156922Z"
    }
   },
   "cell_type": "code",
   "source": "stock_df = make_growth_target_df('datasets/stock_data_for_emm.pkl')",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:35:11.557865Z",
     "start_time": "2024-10-09T12:35:11.485057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stock_df.drop(['index'], inplace=True, axis=1)\n",
    "stock_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             country                        industry currency  \\\n",
       "0             France             Aerospace & Defense      EUR   \n",
       "1            Germany            Software—Application      EUR   \n",
       "2              Italy                   Entertainment      EUR   \n",
       "3              Italy                  Packaged Foods      EUR   \n",
       "4     United Kingdom  Other Precious Metals & Mining      EUR   \n",
       "...              ...                             ...      ...   \n",
       "9774           Japan    Electrical Equipment & Parts      JPY   \n",
       "9775           Japan   Business Equipment & Supplies      JPY   \n",
       "9776           Japan  Electronic Gaming & Multimedia      JPY   \n",
       "9777           Japan           Electronic Components      JPY   \n",
       "9778           Japan              Auto Manufacturers      JPY   \n",
       "\n",
       "     exchangeTimezoneName exchange                  sector  \\\n",
       "0            Europe/Paris      PAR             Industrials   \n",
       "1           Europe/Berlin      GER              Technology   \n",
       "2             Europe/Rome      MIL  Communication Services   \n",
       "3             Europe/Rome      MIL      Consumer Defensive   \n",
       "4           Europe/Berlin      FRA         Basic Materials   \n",
       "...                   ...      ...                     ...   \n",
       "9774        Europe/London      LSE             Industrials   \n",
       "9775        Europe/London      LSE             Industrials   \n",
       "9776        Europe/London      LSE  Communication Services   \n",
       "9777        Europe/London      LSE              Technology   \n",
       "9778        Europe/London      LSE       Consumer Cyclical   \n",
       "\n",
       "      averageVolume10days  enterpriseToEbitda     marketCap  debtToEquity  \\\n",
       "0                   382.0              11.822  5.750674e+07        50.783   \n",
       "1                  8329.0              33.294  1.241838e+09        32.492   \n",
       "2                   330.0             -21.050  2.849466e+07       181.181   \n",
       "3                  3831.0               8.794  5.971590e+07        68.513   \n",
       "4                   108.0               1.402  2.123667e+08        81.212   \n",
       "...                   ...                 ...           ...           ...   \n",
       "9774               4893.0               6.287  2.079737e+10        10.654   \n",
       "9775               1940.0               5.531  5.086319e+09        35.268   \n",
       "9776               3530.0               7.976  6.297095e+09        18.311   \n",
       "9777                350.0               7.200  1.774490e+09         1.943   \n",
       "9778              72760.0              12.413  1.971894e+11       102.678   \n",
       "\n",
       "      fullTimeEmployees                                      growth_target  \n",
       "0                1102.0  [0.0, 1.17, 2.41, 0.41, 2.65, -1.39, -7.24, -4...  \n",
       "1                 650.0  [0.0, -1.84, 17.01, 3.7, -8.24, 8.01, -3.6, 4....  \n",
       "2                  34.0  [0.0, -7.96, -2.61, 0.89, 13.94, -4.85, -6.12,...  \n",
       "3                 232.0  [0.0, -3.39, 0.58, -10.17, 11.33, -2.91, 2.1, ...  \n",
       "4                3474.0  [0.0, 8.5, -15.79, 5.28, 2.24, -1.94, 7.5, -1....  \n",
       "...                 ...                                                ...  \n",
       "9774           145696.0  [0.0, 1.08, 10.44, -11.75, -11.15, 7.96, -9.17...  \n",
       "9775            78360.0  [0.0, 5.82, 1.71, 7.84, -10.55, 3.58, -7.94, 2...  \n",
       "9776             4894.0  [0.0, 6.5, 0.0, -10.85, -1.95, -3.62, -3.95, 9...  \n",
       "9777             1297.0  [0.0, 0.0, 0.0, 0.0, 18.86, 0.0, 0.0, 0.0, 10....  \n",
       "9778           376971.0  [0.0, 2.08, 5.28, -3.71, -7.07, 5.43, -4.35, 4...  \n",
       "\n",
       "[9779 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>industry</th>\n",
       "      <th>currency</th>\n",
       "      <th>exchangeTimezoneName</th>\n",
       "      <th>exchange</th>\n",
       "      <th>sector</th>\n",
       "      <th>averageVolume10days</th>\n",
       "      <th>enterpriseToEbitda</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>fullTimeEmployees</th>\n",
       "      <th>growth_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Europe/Paris</td>\n",
       "      <td>PAR</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>382.0</td>\n",
       "      <td>11.822</td>\n",
       "      <td>5.750674e+07</td>\n",
       "      <td>50.783</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>[0.0, 1.17, 2.41, 0.41, 2.65, -1.39, -7.24, -4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Software—Application</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Europe/Berlin</td>\n",
       "      <td>GER</td>\n",
       "      <td>Technology</td>\n",
       "      <td>8329.0</td>\n",
       "      <td>33.294</td>\n",
       "      <td>1.241838e+09</td>\n",
       "      <td>32.492</td>\n",
       "      <td>650.0</td>\n",
       "      <td>[0.0, -1.84, 17.01, 3.7, -8.24, 8.01, -3.6, 4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Europe/Rome</td>\n",
       "      <td>MIL</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>330.0</td>\n",
       "      <td>-21.050</td>\n",
       "      <td>2.849466e+07</td>\n",
       "      <td>181.181</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[0.0, -7.96, -2.61, 0.89, 13.94, -4.85, -6.12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Packaged Foods</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Europe/Rome</td>\n",
       "      <td>MIL</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>3831.0</td>\n",
       "      <td>8.794</td>\n",
       "      <td>5.971590e+07</td>\n",
       "      <td>68.513</td>\n",
       "      <td>232.0</td>\n",
       "      <td>[0.0, -3.39, 0.58, -10.17, 11.33, -2.91, 2.1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Other Precious Metals &amp; Mining</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Europe/Berlin</td>\n",
       "      <td>FRA</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.402</td>\n",
       "      <td>2.123667e+08</td>\n",
       "      <td>81.212</td>\n",
       "      <td>3474.0</td>\n",
       "      <td>[0.0, 8.5, -15.79, 5.28, 2.24, -1.94, 7.5, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9774</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Electrical Equipment &amp; Parts</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>LSE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>4893.0</td>\n",
       "      <td>6.287</td>\n",
       "      <td>2.079737e+10</td>\n",
       "      <td>10.654</td>\n",
       "      <td>145696.0</td>\n",
       "      <td>[0.0, 1.08, 10.44, -11.75, -11.15, 7.96, -9.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Business Equipment &amp; Supplies</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>LSE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>5.531</td>\n",
       "      <td>5.086319e+09</td>\n",
       "      <td>35.268</td>\n",
       "      <td>78360.0</td>\n",
       "      <td>[0.0, 5.82, 1.71, 7.84, -10.55, 3.58, -7.94, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9776</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Electronic Gaming &amp; Multimedia</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>LSE</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>7.976</td>\n",
       "      <td>6.297095e+09</td>\n",
       "      <td>18.311</td>\n",
       "      <td>4894.0</td>\n",
       "      <td>[0.0, 6.5, 0.0, -10.85, -1.95, -3.62, -3.95, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9777</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Electronic Components</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>LSE</td>\n",
       "      <td>Technology</td>\n",
       "      <td>350.0</td>\n",
       "      <td>7.200</td>\n",
       "      <td>1.774490e+09</td>\n",
       "      <td>1.943</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 18.86, 0.0, 0.0, 0.0, 10....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9778</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>JPY</td>\n",
       "      <td>Europe/London</td>\n",
       "      <td>LSE</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>72760.0</td>\n",
       "      <td>12.413</td>\n",
       "      <td>1.971894e+11</td>\n",
       "      <td>102.678</td>\n",
       "      <td>376971.0</td>\n",
       "      <td>[0.0, 2.08, 5.28, -3.71, -7.07, 5.43, -4.35, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9779 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:35:11.805198Z",
     "start_time": "2024-10-09T12:35:11.667567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = stock_df.values.tolist()\n",
    "column_names = list(stock_df.columns)\n",
    "beam_width = 10\n",
    "beam_depth = 3\n",
    "nr_bins = 8\n",
    "nr_saved = 10\n",
    "subgroup_size = len(data)*0.05\n",
    "target = 'growth_target'\n",
    "window_size = 5"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:35:11.944827Z",
     "start_time": "2024-10-09T12:35:11.884985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_ind = column_names.index(target)\n",
    "all_time_series = [i[target_ind] for i in data]\n",
    "all_time_series = np.array(all_time_series)\n",
    "targets_baseline = np.mean(all_time_series, axis=0)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:35:48.249027Z",
     "start_time": "2024-10-09T12:35:48.220234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "att_indices = list(range(0, len(column_names)))\n",
    "att_indices.remove(target_ind)\n",
    "att_columns = [column_names[i] for i in att_indices]\n",
    "types = categorize_columns_in_order(stock_df, att_columns)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:38:57.481636Z",
     "start_time": "2024-10-09T12:38:11.755226Z"
    }
   },
   "cell_type": "code",
   "source": "results  = beam_search_with_constraint_descriptor2(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size)\n",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:38:57.497570Z",
     "start_time": "2024-10-09T12:38:57.490588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for r in results:\n",
    "    print(r[:3])\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291.80370217535375, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('enterpriseToEbitda', 24.684, <function leeq at 0x000002CACDEC3370>), ('fullTimeEmployees', 1178.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n",
      "(272.6703309513267, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('enterpriseToEbitda', 24.684, <function leeq at 0x000002CACDEC3370>))) 0\n",
      "\n",
      "\n",
      "(270.57615398147425, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('averageVolume10days', 4.0, <function gt at 0x000002CACDEC2560>), ('fullTimeEmployees', 1027.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n",
      "(267.6216412246305, (('marketCap', 1855027770.7673597, <function gt at 0x000002CACDEC2560>), ('marketCap', 62158233600.0, <function leeq at 0x000002CACDEC3370>), ('fullTimeEmployees', 1646.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n",
      "(267.0607424541275, (('marketCap', 1855027770.7673597, <function gt at 0x000002CACDEC2560>), ('averageVolume10days', 0.0, <function gt at 0x000002CACDEC2560>), ('fullTimeEmployees', 1962.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n",
      "(264.7348026110401, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>),)) 0\n",
      "\n",
      "\n",
      "(263.11641592893716, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('enterpriseToEbitda', 24.684, <function leeq at 0x000002CACDEC3370>), ('debtToEquity', 174.297, <function leeq at 0x000002CACDEC3370>))) 0\n",
      "\n",
      "\n",
      "(260.75769389909084, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('marketCap', 50091684389.519356, <function leeq at 0x000002CACDEC3370>), ('fullTimeEmployees', 890.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n",
      "(258.8629853470227, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('marketCap', 50091684389.519356, <function leeq at 0x000002CACDEC3370>))) 0\n",
      "\n",
      "\n",
      "(257.7587667643761, (('marketCap', 730348608.0, <function gt at 0x000002CACDEC2560>), ('averageVolume10days', 0.0, <function gt at 0x000002CACDEC2560>))) 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:36:31.342709Z",
     "start_time": "2024-10-09T12:36:31.336725Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T12:36:31.388588Z",
     "start_time": "2024-10-09T12:36:31.376617Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

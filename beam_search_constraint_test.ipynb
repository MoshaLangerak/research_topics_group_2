{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from queue import PriorityQueue\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "from queue import PriorityQueue\n",
    "from load_data_old import make_growth_target_df\n",
    "import math\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "def gt(a, b):\n",
    "    \"\"\"Returns True if a is greater than b.\"\"\"\n",
    "    return a > b\n",
    "\n",
    "def leeq(a, b):\n",
    "    \"\"\"Returns True if a is less than or equal to b.\"\"\"\n",
    "    return a <= b\n",
    "\n",
    "def eq(a, b):\n",
    "    \"\"\"Returns True if a is equal to b.\"\"\"\n",
    "    return a == b\n",
    "\n",
    "def neq(a, b):\n",
    "    \"\"\"Returns True if a is not equal to b.\"\"\"\n",
    "    return not eq(a, b)\n",
    "\n",
    "def extract_subgroup(descriptors, data, col_index_dict):\n",
    "    \"\"\"Extracts a subgroup of data that matches all provided descriptors.\n",
    "\n",
    "    Args:\n",
    "        descriptors: A list of descriptors, each containing an attribute name, value, and operator.\n",
    "        data: The dataset from which to extract the subgroup.\n",
    "        col_index_dict: A dictionary mapping column names to their indices.\n",
    "\n",
    "    Returns:\n",
    "        A list of rows from the data that match all descriptors.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for row in data:\n",
    "        check = True\n",
    "        for attribute in descriptors:\n",
    "            att_name, descr_value, operator = attribute  # unpack 3 values from attribute\n",
    "            att_index = col_index_dict[att_name]  # get the index for the attribute\n",
    "            value = operator(row[att_index], descr_value)  # apply the operator\n",
    "\n",
    "            if not value:  # if any descriptor does not match\n",
    "                check = False\n",
    "                break\n",
    "\n",
    "        if check:  # if all descriptors match\n",
    "            result.append(row)  # add the row to the result\n",
    "\n",
    "    return result\n",
    "\n",
    "def refin(seed, data, types, nr_bins, descr_indices, index_col_dict, col_index_dict):\n",
    "    \"\"\"Generates new descriptor sets by refining the seed descriptors.\n",
    "\n",
    "    Args:\n",
    "        seed: The initial set of descriptors to refine.\n",
    "        data: The dataset used for extracting subgroups.\n",
    "        types: The types of each column in the dataset.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        descr_indices: The indices of potential descriptors.\n",
    "        index_col_dict: A dictionary mapping index to column names.\n",
    "        col_index_dict: A dictionary mapping column names to indices.\n",
    "\n",
    "    Returns:\n",
    "        A list of new descriptor sets created by refining the seed.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    used_descr = [col_index_dict[i[0]] for i in seed]  # get used descriptors\n",
    "    not_used_indices = descr_indices[:]  # make a copy of descriptor indices\n",
    "    # Filter out indices that are already used or are numeric types\n",
    "    not_used_indices = [i for i in not_used_indices if i not in used_descr or types[i] == \"numeric\"]\n",
    "\n",
    "    for i in not_used_indices:\n",
    "        aux = list(seed)[:]  # copy the seed\n",
    "\n",
    "        if types[i] == 'numeric':\n",
    "            s = extract_subgroup(seed, data, col_index_dict)  # extract subgroup based on seed\n",
    "            all_values = [float(entry[i]) for entry in s]  # get all values for the numeric attribute\n",
    "            all_values = sorted(all_values)  # sort values\n",
    "            n = len(all_values)\n",
    "            # Create split points for binning\n",
    "            split_points = [all_values[math.floor(j * (n/nr_bins))] for j in range(1, nr_bins)]\n",
    "            for s in split_points:\n",
    "                func1 = leeq\n",
    "                func2 = gt\n",
    "\n",
    "                # Create two new descriptors for each split point\n",
    "                local0 = aux[:]\n",
    "                local0.append((index_col_dict[i], s, func1))  # descriptor for less than or equal\n",
    "                res.append(local0)\n",
    "\n",
    "                local1 = aux[:]\n",
    "                local1.append((index_col_dict[i], s, func2))  # descriptor for greater than\n",
    "                res.append(local1)\n",
    "\n",
    "        elif types[i] == 'binary':\n",
    "            func = eq  # equality function for binary descriptors\n",
    "            local0 = aux[:]\n",
    "            local0.append((index_col_dict[i], 0, func))  # descriptor for value 0\n",
    "            local1 = aux[:]\n",
    "            local1.append((index_col_dict[i], 1, func))  # descriptor for value 1\n",
    "            res.append(local0)\n",
    "            res.append(local1)\n",
    "\n",
    "        else:  # nominal attributes\n",
    "            all_values = [entry[i] for entry in data]  # get all unique values\n",
    "            for j in set(all_values):\n",
    "                func1 = eq\n",
    "                local0 = aux[:]\n",
    "                local0.append((index_col_dict[i], j, func1))  # descriptor for equality\n",
    "                res.append(local0)\n",
    "\n",
    "                # descriptor for not equality, not used because gives bad results\n",
    "                # func2 = neq\n",
    "                # local1 = aux[:]\n",
    "                # local1.append((index_col_dict[i], j, func2))\n",
    "                # res.append(local1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def put_item_in_queue(queue, quality, descriptor, size=0):\n",
    "    \"\"\"Adds an item to a priority queue based on its quality.\n",
    "\n",
    "    Args:\n",
    "        queue: The priority queue to which the item will be added.\n",
    "        quality: The quality measure of the item.\n",
    "        descriptor: The descriptor associated with the item.\n",
    "        size: The size of the subgroup represented by the descriptor.\n",
    "    \"\"\"\n",
    "    if queue.full():  # if the queue is full\n",
    "        min_quality, min_descriptor, min_size = queue.get()  # get the lowest quality item\n",
    "        if min_quality >= quality:  # if the new item is not better, put the old one back\n",
    "            queue.put((min_quality, min_descriptor, min_size))\n",
    "        else:  # otherwise, add the new item\n",
    "            queue.put((quality, descriptor, size))\n",
    "    else:\n",
    "        queue.put((quality, descriptor, size))  # add new item to the queue\n",
    "\n",
    "def categorize_columns_in_order(df, att_columns):\n",
    "    \"\"\"Categorizes columns of a DataFrame into numeric, binary, and nominal types.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing the data.\n",
    "        att_columns: The columns to categorize.\n",
    "\n",
    "    Returns:\n",
    "        A list of column types corresponding to the provided attribute columns.\n",
    "    \"\"\"\n",
    "    column_types = []  # List to store the categories in order\n",
    "\n",
    "    for col in att_columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):  # Check if the column is numeric\n",
    "            column_types.append('numeric')\n",
    "        elif df[col].nunique() == 2:  # Check for binary columns\n",
    "            column_types.append('binary')\n",
    "        else:  # Otherwise, treat it as nominal\n",
    "            column_types.append('nominal')\n",
    "\n",
    "    return column_types\n",
    "\n",
    "def make_rolling_windows(growth_target, window_size):\n",
    "    \"\"\"Creates rolling windows for the target data.\n",
    "\n",
    "    Args:\n",
    "        growth_target: The target data for which to create rolling windows.\n",
    "        window_size: The size of each window.\n",
    "\n",
    "    Returns:\n",
    "        A new array of rolling windows.\n",
    "    \"\"\"\n",
    "    return np.lib.stride_tricks.sliding_window_view(growth_target, window_shape=window_size)[::window_size]\n",
    "\n",
    "def quality_measure(targets_subgroup, targets_baseline,\n",
    "                    aggregate_func_window=np.mean, aggregate_func=np.max):\n",
    "    \"\"\"Calculates a quality measure for a subgroup compared to a baseline.\n",
    "\n",
    "    Args:\n",
    "        targets_subgroup: list with timeseries where the timeseries are divided in windows (list with lists with lists)\n",
    "        targets_baseline: list with baseline target values in windows (list with lists).\n",
    "        aggregate_func_window: Function to aggregate over windows (default: mean).\n",
    "        aggregate_func: Function to aggregate the final quality measure (default: max).\n",
    "\n",
    "    Returns:\n",
    "        A quality score representing the difference between the subgroup and baseline.\n",
    "    \"\"\"\n",
    "    # Aggregate points in each window for each timeseries in the subgroup\n",
    "    subgroup_aggregated_windows = aggregate_func_window(targets_subgroup, axis=2)\n",
    "\n",
    "    # Calculate mean values for the baseline and subgroup for each window\n",
    "    baseline_means = np.mean(targets_baseline, axis=1)\n",
    "    subgroup_means = np.mean(subgroup_aggregated_windows, axis=0)\n",
    "\n",
    "    # Calculate absolute differences between means for each window\n",
    "    abs_diff_mean = np.abs(subgroup_means - baseline_means)\n",
    "\n",
    "    # Calculate standard error of the subgroup for each window\n",
    "    subgroup_std = np.std(subgroup_aggregated_windows, axis=0)\n",
    "    standard_error_subgroup = subgroup_std #/ np.sqrt(len(targets_subgroup))\n",
    "\n",
    "    # Calculate z-scores\n",
    "    z_scores = np.divide(abs_diff_mean, standard_error_subgroup, where=standard_error_subgroup != 0)\n",
    "\n",
    "    # Calculate the final quality score\n",
    "    quality_score = aggregate_func(z_scores)\n",
    "\n",
    "    return quality_score\n",
    "\n",
    "\n",
    "def filter_df_on_descriptors(df, descriptors):\n",
    "    \"\"\"Filters a DataFrame based on specified descriptors.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to filter.\n",
    "        descriptors: A list of descriptors, each containing an attribute name, value, and operator.\n",
    "\n",
    "    Returns:\n",
    "        A filtered DataFrame where all descriptors hold true.\n",
    "    \"\"\"\n",
    "    # Loop through each descriptor to apply filtering\n",
    "    for desc in descriptors:\n",
    "        # Apply the operator defined in the descriptor to filter the DataFrame\n",
    "        df = df[df[desc[0]].apply(lambda x: desc[2](x, desc[1]))]  # Filter based on the descriptor\n",
    "    return df  # Return the filtered DataFrame\n",
    "\n",
    "def get_all_descriptors(pq, index=1):\n",
    "    \"\"\"Retrieve all descriptors from a priority queue without altering its contents.\n",
    "\n",
    "    Args:\n",
    "        pq: A priority queue containing descriptor tuples.\n",
    "        index: The index of item to retrieve\n",
    "\n",
    "    Returns:\n",
    "        A list with info out of the results pq\n",
    "    \"\"\"\n",
    "    temp_items = []  # Temporary list to store all items from the queue\n",
    "    info = []  # List to store info of results pq (descriptors, quality, etc.)\n",
    "\n",
    "    # Retrieve all items from the queue\n",
    "    while not pq.empty():\n",
    "        item = pq.get()  # Get item from the queue\n",
    "        temp_items.append(item)  # Store the item temporarily\n",
    "        info.append(item[index])  # Extract and store the descriptor\n",
    "\n",
    "    # Put all items back into the queue to maintain its original state\n",
    "    for item in temp_items:\n",
    "        pq.put(item)\n",
    "\n",
    "    return info\n",
    "def beam_search_with_constraint_paper(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins, nr_saved, subgroup_size, target, types, window_size, max_subgroup_size=100000):\n",
    "    \"\"\"Performs beam search with a constraint to avoid adding similar descriptors.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to analyze.\n",
    "        targets_baseline: The baseline target values for comparison.\n",
    "        column_names: The names of the columns in the dataset.\n",
    "        beam_width: The number of descriptors to keep at each depth level.\n",
    "        beam_depth: The maximum depth of the beam search.\n",
    "        nr_bins: The number of bins to create for numeric attributes.\n",
    "        nr_saved: The number of best results to save.\n",
    "        subgroup_size: The minimum size of a subgroup to consider.\n",
    "        target: The target variable for which to evaluate subgroups.\n",
    "        types: The types of each column in the dataset (e.g., numeric, binary, nominal).\n",
    "        window_size: The size of the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        A list of the best descriptor sets found during the search, ensuring no similar descriptors.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for indexing columns by name and vice versa\n",
    "    index_col_dict = {i: col for i, col in enumerate(column_names)}\n",
    "    col_index_dict = {col: i for i, col in enumerate(column_names)}\n",
    "    target_ind = column_names.index(target)  # Get index of the target column\n",
    "    att_indices = list(range(len(column_names)))  # Create a list of all indices\n",
    "    att_indices.remove(target_ind)  # Remove the target index from attribute indices\n",
    "\n",
    "    # Prepare data windows with rolling windows for the target variable\n",
    "    data_windows = []\n",
    "    for row in data:\n",
    "        new_row = row[:]  # Create a copy of the row\n",
    "        new_row[target_ind] = make_rolling_windows(row[target_ind], window_size)  # Apply rolling window\n",
    "        data_windows.append(new_row)  # Add the new row to data_windows\n",
    "\n",
    "    # Update the data and baseline targets to use rolling windows\n",
    "    data = data_windows\n",
    "    targets_baseline = make_rolling_windows(targets_baseline, window_size)\n",
    "\n",
    "    # Initialize a deque for the beam search and a priority queue for results\n",
    "    beam_queue = deque([()])  # Start with an empty seed\n",
    "    results = PriorityQueue(nr_saved)  # Queue to hold the best results\n",
    "    results.put((0, [(0, 0, 0)], 0))  # Add a dummy descriptor to initialize\n",
    "\n",
    "    # Iterate through each depth of the beam search\n",
    "    for depth in range(beam_depth):\n",
    "        beam = PriorityQueue(beam_width)  # Initialize a new beam for this depth\n",
    "\n",
    "        # While there are seeds in the beam queue\n",
    "        while bool(beam_queue):\n",
    "            seed = beam_queue.popleft()  # Get the next seed descriptor\n",
    "            descriptor_set = refin(seed, data, types, nr_bins, att_indices, index_col_dict, col_index_dict)  # Refine descriptors based on seed\n",
    "\n",
    "            # Evaluate each descriptor set generated\n",
    "            for descriptor in descriptor_set:\n",
    "                subgroup = extract_subgroup(descriptor, data, col_index_dict)  # Extract subgroup for the current descriptor\n",
    "                if len(subgroup) >= subgroup_size and len(subgroup) < max_subgroup_size:  # Ensure subgroup is large enough and descriptor is not similar\n",
    "                    targets_subgroup = [i[target_ind] for i in subgroup]  # Extract target values for the subgroup\n",
    "                    quality_result = quality_measure(targets_subgroup, targets_baseline)  # Calculate quality measure\n",
    "                    if not descriptors_similar_paper(quality_result, descriptor, results): # check if there are already subgroups with similar descriptors\n",
    "                        put_item_in_queue(results, quality_result, tuple(descriptor), len(subgroup))  # Add to results queue\n",
    "                        put_item_in_queue(beam, quality_result, tuple(descriptor))  # Add to the current beam\n",
    "\n",
    "        # After processing the beam, update the beam queue with new combinations\n",
    "        while not beam.empty():\n",
    "            new_combination = beam.get()  # Get the highest quality descriptor from the beam\n",
    "            new_combination = new_combination[1]  # Extract the descriptor from the tuple\n",
    "            beam_queue.append(new_combination)  # Add it to the next depth of the beam search\n",
    "\n",
    "    # Compile results into a list and reverse to have the best results first\n",
    "    results_list = []\n",
    "    while not results.empty():\n",
    "        item = results.get()  # Get items from the results queue\n",
    "        results_list.append(item)  # Add to the results list\n",
    "    results_list.reverse()  # Reverse the list to have best results first\n",
    "\n",
    "    return results_list  # Return the list of best descriptor sets\n",
    "\n",
    "def descriptors_similar_paper(quality, descriptor1, pq):\n",
    "\n",
    "    if len(descriptor1) == 1:\n",
    "        return False\n",
    "\n",
    "    tolerance = 0.1\n",
    "    desc1_dict = {metric: (value, func) for metric, value, func in descriptor1}\n",
    "    descriptor_list = get_all_descriptors(pq, 1)\n",
    "    quality_list = get_all_descriptors(pq, 0)\n",
    "\n",
    "    # Early exit if quality difference exceeds threshold\n",
    "    if min(abs(quality - q) for q in quality_list) > 1000:\n",
    "        return False\n",
    "\n",
    "    # Compare against each descriptor in the queue\n",
    "    for descriptor2 in descriptor_list:\n",
    "\n",
    "        desc2_dict = {metric: (value, func) for metric, value, func in descriptor2}\n",
    "\n",
    "        match_count = 0\n",
    "\n",
    "        for metric, (value1, func1) in desc1_dict.items():\n",
    "            if metric not in desc2_dict:\n",
    "                continue  # If a metric is missing, no need to continue\n",
    "\n",
    "            value2, func2 = desc2_dict[metric]\n",
    "\n",
    "            # Function mismatch, skip this descriptor\n",
    "            if func1 != func2:\n",
    "                continue\n",
    "\n",
    "            # Numeric comparison within tolerance\n",
    "            if isinstance(value1, (int, float)) and isinstance(value2, (int, float)):\n",
    "                if abs(value1 - value2) > abs(tolerance * value1):\n",
    "                    continue  # Values out of tolerance range\n",
    "\n",
    "            # String comparison for non-numeric values\n",
    "            elif isinstance(value1, str) and value1 != value2:\n",
    "                continue  # String values don't match\n",
    "\n",
    "            match_count += 1\n",
    "\n",
    "            # Early exit when all metrics except 1 match\n",
    "            if match_count >= len(descriptor1)-1:\n",
    "                return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stock_df = make_growth_target_df('datasets/stock_data_for_emm.pkl')\n",
    "#stock_df.drop(['index'], inplace=True, axis=1)\n",
    "stock_df"
   ],
   "id": "e067637d5c231d62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = stock_df.values.tolist()\n",
    "column_names = list(stock_df.columns)\n",
    "beam_width = 10\n",
    "beam_depth = 3\n",
    "nr_bins = 8\n",
    "nr_saved = 10\n",
    "subgroup_size = len(data) * 0.05\n",
    "target = 'growth_target'\n",
    "window_size = 5\n",
    "target_ind = column_names.index(target)\n",
    "all_time_series = [i[target_ind] for i in data]\n",
    "all_time_series = np.array(all_time_series)\n",
    "targets_baseline = np.mean(all_time_series, axis=0)\n",
    "att_indices = list(range(0, len(column_names)))\n",
    "att_indices.remove(target_ind)\n",
    "att_columns = [column_names[i] for i in att_indices]\n",
    "types = categorize_columns_in_order(stock_df, att_columns)\n",
    "results = beam_search_with_constraint_paper(data, targets_baseline, column_names, beam_width, beam_depth, nr_bins,\n",
    "                                                  nr_saved, subgroup_size, target, types, window_size)\n"
   ],
   "id": "407bd4c92dabb933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for r in results:\n",
    "    print(r[:3])\n",
    "    print('\\n')"
   ],
   "id": "c248b4f526e07c30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
